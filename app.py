# app.py
# -*- coding: utf-8 -*-
import time
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
import ta
import requests
import certifi
from datetime import datetime, timedelta, timezone

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False

# ----------------------------
# ÂÖ®Â±ÄËÆæÁΩÆ
# ----------------------------
st.set_page_config(page_title="Âä†ÂØÜË¥ßÂ∏ÅÂ§öÊ®°ÂûãÈ¢ÑÊµã (OKX+LSTM/GRU/Transformer/TCN)", layout="wide")
plt.rcParams.update({"figure.autolayout": True})

def set_seed(seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
set_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------
# ‰æßËæπÊ†èÂèÇÊï∞
# ----------------------------
st.sidebar.header("ÈÖçÁΩÆÂèÇÊï∞")
st.sidebar.caption("Êï∞ÊçÆÊ∫êÔºöOKX v5 ÂÖ¨ÂÖ±Ë°åÊÉÖ APIÔºàÂÖç KeyÔºâ")
inst_id = st.sidebar.text_input("‰∫§ÊòìÂØπÔºà‰æãÂ¶ÇÔºöBTC-USDT / ETH-USDT / SOL-USDTÔºâ", value="BTC-USDT")
start_date = st.sidebar.date_input("Ëµ∑ÂßãÊó•Êúü", value=pd.to_datetime("2018-01-01"))
bar = st.sidebar.selectbox("KÁ∫øÂë®Êúü", options=["1D", "4H", "1H"], index=0)  # OKX ÊîØÊåÅÁöÑÂ∏∏Áî®Âë®Êúü
use_proxy = st.sidebar.checkbox("‰ΩøÁî® HTTP ‰ª£ÁêÜÔºàÂèØÈÄâÔºâ", value=False)
proxy_url = st.sidebar.text_input("‰ª£ÁêÜÂú∞ÂùÄÔºàÁ§∫‰æãÔºöhttp://127.0.0.1:7897Ôºâ", value="http://127.0.0.1:7897") if use_proxy else None

seq_len = st.sidebar.number_input("ËæìÂÖ•Á™óÂè£ÈïøÂ∫¶Ôºàbar Êï∞Ôºâ", min_value=20, max_value=240, value=60, step=5)
max_h = st.sidebar.selectbox("È¢ÑÊµãÊ≠•Êï∞ÔºàËæìÂá∫ÈïøÂ∫¶Ôºâ", options=[5, 10, 15], index=2)
epochs = st.sidebar.slider("ËÆ≠ÁªÉËΩÆÊ¨°", min_value=10, max_value=200, value=60, step=10)
batch_size = st.sidebar.selectbox("Batch Size", options=[32, 64, 128], index=1)
hidden_size = st.sidebar.selectbox("ÈöêËóèÂ±ÇÁª¥Â∫¶ÔºàRNN/TCN/TransformerÔºâ", options=[64, 128, 192], index=0)
nhead = st.sidebar.selectbox("Transformer Â§öÂ§¥Êï∞", options=[2, 4, 8], index=1)
dropout = st.sidebar.slider("Dropout", min_value=0.0, max_value=0.5, value=0.2, step=0.05)
train_ratio = st.sidebar.slider("ËÆ≠ÁªÉÈõÜÊØî‰æã", min_value=0.5, max_value=0.95, value=0.8, step=0.05)
horizons_eval = st.sidebar.multiselect("ËØÑ‰º∞Âë®ÊúüÔºàÂèØÂ§öÈÄâÔºâ", [1, 3, 5, 10, 15], default=[1, 3, 5, 10, 15])

models_to_run = st.sidebar.multiselect(
    "ÈÄâÊã©Ê®°ÂûãÔºàËá≥Â∞ë‰∏Ä‰∏™Ôºâ",
    ["LSTM", "GRU", "Transformer", "TCN"],
    default=["LSTM", "GRU", "Transformer", "TCN"],
)

st.sidebar.markdown("---")
run_button = st.sidebar.button("üöÄ ÂºÄÂßãÈ¢ÑÊµã")

# ----------------------------
# Â∑•ÂÖ∑ & Êï∞ÊçÆÊäìÂèñÔºàOKX APIÔºâ
# ----------------------------
def _safe_series(x):
    return pd.Series(np.array(x).squeeze(), index=x.index if hasattr(x, "index") else None)

@st.cache_data(show_spinner=True)
def fetch_okx_candles(inst_id: str, start_date: pd.Timestamp, bar: str = "1D", proxy_url: str | None = None) -> pd.DataFrame:
    import requests, certifi, time
    import pandas as pd
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    session.verify = certifi.where()
    if proxy_url:
        session.proxies.update({"http": proxy_url, "https": proxy_url})

    base = "https://www.okx.com"
    path_hist = "/api/v5/market/history-candles"  # ‚úÖ ÂéÜÂè≤Á´ØÁÇπ
    path_curr = "/api/v5/market/candles"          # ËøëÊúüÁ´ØÁÇπÔºàÂèØÈÄâÔºâ
    per_page = 100
    max_loops = 2000  # Ë∂≥Â§üÂ§ß
    start_ms = int(pd.to_datetime(start_date).tz_localize("UTC").timestamp() * 1000)

    def fetch_page(path, params):
        r = session.get(base + path, params=params, timeout=20)
        r.raise_for_status()
        js = r.json()
        if js.get("code") != "0":
            raise RuntimeError(js)
        return js.get("data", [])

    candles = []
    before = None

    # 1) ÂÖàÁî® history-candles ‰∏çÊñ≠Âêë‚ÄúÊõ¥Êó©‚ÄùÁøªÈ°µ
    for _ in range(max_loops):
        params = {"instId": inst_id, "bar": bar, "limit": str(per_page)}
        if before is not None:
            params["before"] = str(before)
        arr = fetch_page(path_hist, params)
        if not arr:
            break
        for row in arr:
            ts = int(row[0])
            o, h, l, c = map(float, row[1:5])
            vol = float(row[5])
            candles.append((ts, o, h, l, c, vol))
        earliest_ts = int(arr[-1][0])
        if earliest_ts <= start_ms:  # Âà∞ËææÊàñÊó©‰∫éËµ∑ÂßãÊó∂Èó¥Â∞±Â§ü‰∫Ü
            break
        before = earliest_ts - 1
        time.sleep(0.1)

    # ÔºàÂèØÈÄâÔºâ2) ÂÜçÁî® /candles Ë°•ÊúÄËøë‰∏ÄÊÆµÔºåÈò≤Ê≠¢ history Êúâ‰∫õ‰∫§ÊòìÂØπÊúÄËøëÈ°µÂª∂Ëøü
    params = {"instId": inst_id, "bar": bar, "limit": str(per_page)}
    arr = fetch_page(path_curr, params)
    for row in arr:
        ts = int(row[0])
        o, h, l, c = map(float, row[1:5])
        vol = float(row[5])
        candles.append((ts, o, h, l, c, vol))

    if not candles:
        return pd.DataFrame()

    # ÂéªÈáç & Ê≠£Â∫è & ËøáÊª§
    df = pd.DataFrame(candles, columns=["ts", "Open", "High", "Low", "Close", "Volume"]).drop_duplicates("ts")
    df = df.sort_values("ts").reset_index(drop=True)
    df = df[df["ts"] >= start_ms]
    df["Date"] = pd.to_datetime(df["ts"], unit="ms", utc=True).dt.tz_convert(None)
    df = df.set_index("Date")
    return df[["Open", "High", "Low", "Close", "Volume"]]


@st.cache_data(show_spinner=True)
def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:
    df = raw_df[["Open", "High", "Low", "Close", "Volume"]].copy()
    df["daily_return"] = df["Close"].pct_change()
    df["MA5"] = df["Close"].rolling(window=5).mean()
    df["MA10"] = df["Close"].rolling(window=10).mean()
    df["MA20"] = df["Close"].rolling(window=20).mean()
    df["volatility"] = df["daily_return"].rolling(window=10).std()

    close = _safe_series(df["Close"])
    volume = _safe_series(df["Volume"])
    df["RSI"] = ta.momentum.RSIIndicator(close, window=14).rsi()
    df["OBV"] = ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()
    df = df.dropna().copy()
    return df

@st.cache_data(show_spinner=True)
def scale_data(df: pd.DataFrame):
    features = df.drop(columns=["Close"])
    target = df[["Close"]]
    feature_scaler = MinMaxScaler()
    target_scaler = MinMaxScaler()
    scaled_features = feature_scaler.fit_transform(features)
    scaled_target = target_scaler.fit_transform(target)
    scaled_data = np.concatenate([scaled_features, scaled_target], axis=1)
    return scaled_data, feature_scaler, target_scaler

def create_sequences_multiday(scaled_data: np.ndarray, seq_len: int, max_h: int):
    xs, ys, bases = [], [], []
    for i in range(len(scaled_data) - seq_len - max_h):
        x = scaled_data[i:i + seq_len]
        base = scaled_data[i + seq_len - 1, -1]
        future = [scaled_data[i + seq_len + h, -1] for h in range(1, max_h + 1)]
        xs.append(x); ys.append(future); bases.append(base)
    return np.array(xs), np.array(ys), np.array(bases)

class SeqDataset(Dataset):
    def __init__(self, X, Y):
        self.x = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(Y, dtype=torch.float32)
    def __len__(self): return len(self.x)
    def __getitem__(self, idx): return self.x[idx], self.y[idx]

# ----------------------------
# Ê®°ÂûãÂÆö‰πâ
# ----------------------------
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=15, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.num_layers = num_layers; self.hidden_size = hidden_size
    def forward(self, x):
        B = x.size(0)
        h0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)
        out, _ = self.lstm(x, (h0, c0))
        return self.fc(out[:, -1, :])

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=15, dropout=0.2):
        super().__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.num_layers = num_layers; self.hidden_size = hidden_size
    def forward(self, x):
        B = x.size(0)
        h0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)
        out, _ = self.gru(x, h0)
        return self.fc(out[:, -1, :])

class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_size, num_layers=2, nhead=4, hidden_dim=128, output_size=15, dropout=0.1):
        super().__init__()
        self.d_model = hidden_dim
        self.input_proj = nn.Linear(input_size, hidden_dim)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim * 2,
            dropout=dropout, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.fc = nn.Linear(hidden_dim, output_size)
    def _positional_encoding(self, L, D, device):
        pos = torch.arange(0, L, device=device).unsqueeze(1)
        i = torch.arange(0, D, 2, device=device)
        angle = 1.0 / torch.pow(10000, (i.float() / D))
        pe = torch.zeros(L, D, device=device)
        pe[:, 0::2] = torch.sin(pos * angle); pe[:, 1::2] = torch.cos(pos * angle)
        return pe.unsqueeze(0)
    def forward(self, x):
        B, T, _ = x.shape
        z = self.input_proj(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32, device=x.device))
        z = z + self._positional_encoding(T, self.d_model, x.device)
        out = self.encoder(z)[:, -1, :]
        return self.fc(out)

class Chomp1d(nn.Module):
    def __init__(self, chomp_size): super().__init__(); self.chomp_size = chomp_size
    def forward(self, x): return x[:, :, :-self.chomp_size].contiguous()

class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, ksz, stride, dilation, padding, dropout=0.2):
        super().__init__()
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, ksz, stride=stride, padding=padding, dilation=dilation)
        self.chomp1 = Chomp1d(padding); self.relu1 = nn.ReLU(); self.drop1 = nn.Dropout(dropout)
        self.conv2 = nn.Conv1d(n_outputs, n_outputs, ksz, stride=stride, padding=padding, dilation=dilation)
        self.chomp2 = Chomp1d(padding); self.relu2 = nn.ReLU(); self.drop2 = nn.Dropout(dropout)
        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.drop1,
                                 self.conv2, self.chomp2, self.relu2, self.drop2)
        self.down = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        for m in self.net:
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
    def forward(self, x):
        out = self.net(x); res = x if self.down is None else self.down(x)
        return self.relu(out + res)

class TCN(nn.Module):
    def __init__(self, input_size, output_size=15, num_channels=[64, 64, 128], kernel_size=3, dropout=0.2):
        super().__init__()
        layers = []
        for i, ch in enumerate(num_channels):
            dilation = 2 ** i
            in_ch = input_size if i == 0 else num_channels[i - 1]
            padding = (kernel_size - 1) * dilation
            layers.append(TemporalBlock(in_ch, ch, kernel_size, 1, dilation, padding, dropout))
        self.net = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], output_size)
    def forward(self, x):
        z = x.transpose(1, 2)
        y = self.net(z)
        y = y[:, :, -1]
        return self.fc(y)

# ----------------------------
# ËÆ≠ÁªÉ‰∏éËØÑ‰º∞
# ----------------------------
def inv_transform_each_day(arr_2d, scaler: MinMaxScaler):
    out = np.zeros_like(arr_2d, dtype=float)
    for k in range(arr_2d.shape[1]):
        out[:, k] = scaler.inverse_transform(arr_2d[:, [k]]).ravel()
    return out

def trend_accuracy_horizon(true_seq, pred_seq, base_price, horizon):
    true_dir = np.sign(true_seq[:, :horizon] - base_price[:, None])
    pred_dir = np.sign(pred_seq[:, :horizon] - base_price[:, None])
    return (true_dir == pred_dir).mean()

@st.cache_resource(show_spinner=False)
def build_model(name, input_size, output_size, hidden_size, nhead, dropout):
    if name == "LSTM":
        return LSTMModel(input_size, hidden_size=hidden_size, num_layers=2, output_size=output_size, dropout=dropout).to(device)
    if name == "GRU":
        return GRUModel(input_size, hidden_size=hidden_size, num_layers=2, output_size=output_size, dropout=dropout).to(device)
    if name == "Transformer":
        return TimeSeriesTransformer(input_size, num_layers=2, nhead=nhead, hidden_dim=hidden_size, output_size=output_size, dropout=dropout).to(device)
    if name == "TCN":
        return TCN(input_size, output_size=output_size, num_channels=[hidden_size, hidden_size, hidden_size * 2], kernel_size=3, dropout=dropout).to(device)
    raise ValueError("unknown model")

def train_and_evaluate(model_name, model, X_train, Y_train, X_test, Y_test, base_test, target_scaler, epochs, batch_size):
    start = time.time()
    loader = DataLoader(SeqDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    model.train(); losses = []
    for ep in range(epochs):
        total = 0.0
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            total += loss.item()
        losses.append(total / max(1, len(loader)))
    train_time = time.time() - start

    model.eval()
    with torch.no_grad():
        Xtt = torch.tensor(X_test, dtype=torch.float32, device=device)
        preds_scaled = model(Xtt).cpu().numpy()

    preds = inv_transform_each_day(preds_scaled, target_scaler)
    true = inv_transform_each_day(Y_test, target_scaler)
    base_true = target_scaler.inverse_transform(base_test.reshape(-1, 1)).ravel()

    rows = []
    for H in horizons_eval:
        t = true[:, :H].ravel()
        p = preds[:, :H].ravel()
        rmse = math.sqrt(mean_squared_error(t, p))
        mae = mean_absolute_error(t, p)
        r2 = r2_score(t, p) if len(np.unique(t)) > 1 else np.nan
        acc = trend_accuracy_horizon(true, preds, base_true, H)
        rows.append([model_name, H, rmse, mae, r2, acc, train_time])
    return rows, preds, true, losses

# ----------------------------
# ‰∏ª‰ΩìÊµÅÁ®ã
# ----------------------------
st.title("ü™ô Âä†ÂØÜË¥ßÂ∏ÅÂ§öÊ®°ÂûãÈ¢ÑÊµãÔºàOKX / LSTM / GRU / Transformer / TCNÔºâ")
st.caption("ËæìÂÖ•‰∫§ÊòìÂØπ‰∏éÂèÇÊï∞ÔºåËá™Âä®ÂÆåÊàêÊï∞ÊçÆÊäìÂèñ„ÄÅËÆ≠ÁªÉ„ÄÅÈ¢ÑÊµã„ÄÅÂØπÊØî‰∏éÂª∫ËÆÆ„ÄÇÔºà‰ªÖÁî®‰∫éÁ†îÁ©∂‰∏éÊïôÂ≠¶Ôºå‰∏çÊûÑÊàêÊäïËµÑÂª∫ËÆÆÔºâ")

if run_button:
    with st.spinner("Ê≠£Âú®‰ªé OKX Ëé∑ÂèñÊï∞ÊçÆÂπ∂ÊûÑÂª∫ÁâπÂæÅ..."):
        raw = fetch_okx_candles(inst_id, start_date, bar, proxy_url if use_proxy else None)
        if raw.empty:
            st.error("Ëé∑ÂèñÊï∞ÊçÆÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•‰∫§ÊòìÂØπÊòØÂê¶Â≠òÂú®ÊàñÁΩëÁªú/‰ª£ÁêÜËÆæÁΩÆ„ÄÇÁ§∫‰æãÔºöBTC-USDT„ÄÅETH-USDT„ÄÅSOL-USDT")
            st.stop()
        feats = build_features(raw)
        scaled_data, feature_scaler, target_scaler = scale_data(feats)
        X, Y, base_close = create_sequences_multiday(scaled_data, seq_len, max_h)
        st.write(
            "raw rows:", len(raw),
            "feats rows:", len(feats),
            "scaled rows:", len(scaled_data),
            "need at least:", 100 + seq_len + max_h,
            "=> len(X):", len(X)
        )
        if len(X) < 100:
            st.warning("Ê†∑Êú¨Â§™Â∞ëÔºåÊó†Ê≥ïËÆ≠ÁªÉ„ÄÇËØ∑Áº©Áü≠Ëµ∑ÂßãÊó•ÊúüÊàñÂáèÂ∞ë seq_len / max_h„ÄÇ")
            st.stop()

        split = int(len(X) * train_ratio)
        X_train, X_test = X[:split], X[split:]
        Y_train, Y_test = Y[:split], Y[split:]
        base_train, base_test = base_close[:split], base_close[split:]

    st.success(f"Êï∞ÊçÆÂáÜÂ§áÂÆåÊàêÔºöËÆ≠ÁªÉÊ†∑Êú¨ {len(X_train)}ÔºåÊµãËØïÊ†∑Êú¨ {len(X_test)}„ÄÇÔºàÊï∞ÊçÆÊ∫êÔºöOKX {bar} KÁ∫øÔºâ")

    # ÂéÜÂè≤‰ª∑Ê†º
    with st.expander("Êü•ÁúãÂéÜÂè≤‰ª∑Ê†ºÊõ≤Á∫ø", expanded=False):
        fig, ax = plt.subplots(figsize=(8, 3))
        ax.plot(feats.index, feats["Close"])
        ax.set_title(f"{inst_id} ÂéÜÂè≤Êî∂Áõò‰ª∑Ôºà{bar}Ôºâ")
        ax.set_xlabel("Date"); ax.set_ylabel("Close")
        st.pyplot(fig)

    all_results = []; model_preds = {}
    cols = st.columns(len(models_to_run))
    for i, name in enumerate(models_to_run):
        with cols[i]:
            st.markdown(f"### {name}")
            model = build_model(
                name,
                input_size=scaled_data.shape[1],
                output_size=max_h,
                hidden_size=hidden_size,
                nhead=nhead,
                dropout=dropout,
            )
            rows, preds, true, losses = train_and_evaluate(
                name, model,
                X_train, Y_train, X_test, Y_test,
                base_test, target_scaler,
                epochs, batch_size
            )
            all_results += rows
            model_preds[name] = (preds, true, losses)

            fig_l, ax_l = plt.subplots(figsize=(4, 2.2))
            ax_l.plot(losses); ax_l.set_title("ËÆ≠ÁªÉÊçüÂ§±")
            ax_l.set_xlabel("Epoch"); ax_l.set_ylabel("MSE")
            st.pyplot(fig_l)

    result_df = pd.DataFrame(
        all_results, columns=["Model", "Horizon", "RMSE", "MAE", "R2", "TrendAcc", "TrainTime(s)"]
    )
    st.subheader("üìä ÊåáÊ†áÂØπÊØî")
    st.dataframe(
        result_df.style.format({
            "RMSE": "{:.4f}", "MAE": "{:.4f}", "R2": "{:.4f}",
            "TrendAcc": "{:.4f}", "TrainTime(s)": "{:.2f}",
        }),
        use_container_width=True,
    )

    c1, c2 = st.columns(2)
    with c1:
        fig1, ax1 = plt.subplots(figsize=(6, 3))
        for m in result_df["Model"].unique():
            dfm = result_df[result_df["Model"] == m]
            ax1.plot(dfm["Horizon"], dfm["TrendAcc"], marker="o", label=m)
        ax1.set_title("ÂêÑÊ®°ÂûãË∂ãÂäøÂáÜÁ°ÆÁéáÂØπÊØî"); ax1.set_xlabel("Horizon (bars)"); ax1.set_ylabel("TrendAcc")
        ax1.legend(); st.pyplot(fig1)
    with c2:
        fig2, ax2 = plt.subplots(figsize=(6, 3))
        for m in result_df["Model"].unique():
            dfm = result_df[result_df["Model"] == m]
            ax2.plot(dfm["Horizon"], dfm["RMSE"], marker="o", label=m)
        ax2.set_title("ÂêÑÊ®°Âûã RMSE ÂØπÊØî"); ax2.set_xlabel("Horizon (bars)"); ax2.set_ylabel("RMSE")
        ax2.legend(); st.pyplot(fig2)

    st.subheader("üî≠ ÊµãËØïÈõÜÊ†∑Êú¨Êú™Êù•Ë∑ØÂæÑÂØπÊØîÔºàÈöèÊú∫Á§∫‰æãÔºâ")
    if len(X_test) > 0:
        idx = np.random.randint(0, len(X_test))
        figp, axp = plt.subplots(figsize=(8, 3))
        for name, (preds, true, _) in model_preds.items():
            axp.plot(range(1, max_h + 1), true[idx], marker="o", label=f"{name}-True")
            axp.plot(range(1, max_h + 1), preds[idx], marker="o", linestyle="--", label=f"{name}-Pred")
        axp.set_title(f"Ê†∑Êú¨ {idx} ÁöÑÊú™Êù• {max_h} ‰∏™ {bar}ÔºàÁúüÂÆû vs È¢ÑÊµãÔºâ")
        axp.set_xlabel("Steps Ahead"); axp.set_ylabel("Close")
        axp.legend(); st.pyplot(figp)

    st.subheader("üß≠ Êú™Êù•Âª∫ËÆÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºå‰∏çÊûÑÊàêÊäïËµÑÂª∫ËÆÆÔºâ")
    suggestions = []
    for m in models_to_run:
        preds, true, _ = model_preds[m]
        path = preds[-1]
        slope = (path[-1] - path[0]) / max(1, (len(path) - 1))
        df_m = result_df[(result_df["Model"] == m) & (result_df["Horizon"] == max_h)]
        trend_acc = float(df_m["TrendAcc"].iloc[0]) if len(df_m) > 0 else np.nan
        if slope > 0 and trend_acc >= 0.55:
            view = "ÂÅèÂ§öÔºà‰∏äË°åÊ¶ÇÁéáËæÉÈ´òÔºâ"
        elif slope < 0 and trend_acc >= 0.55:
            view = "ÂÅèÁ©∫Ôºà‰∏ãË°åÊ¶ÇÁéáËæÉÈ´òÔºâ"
        else:
            view = "‰∏≠ÊÄßÔºà‰ø°Âè∑ËæÉÂº±Ôºâ"
        suggestions.append({"Model": m, "ForecastSlope": slope, f"TrendAcc@{max_h}": trend_acc, "View": view})

    sug_df = pd.DataFrame(suggestions)
    st.dataframe(sug_df.style.format({"ForecastSlope": "{:.4f}", f"TrendAcc@{max_h}": "{:.3f}"}), use_container_width=True)

    votes = {"ÂÅèÂ§ö": 0.0, "ÂÅèÁ©∫": 0.0, "‰∏≠ÊÄß": 0.0}
    for r in suggestions:
        votes[r["View"].split("Ôºà")[0]] += max(0.0, (r.get(f"TrendAcc@{max_h}") or 0) - 0.5)
    final_view = max(votes, key=votes.get) if sum(votes.values()) > 0 else "‰∏≠ÊÄß"
    st.info(f"**ÁªºÂêàÂèÇËÄÉËßÇÁÇπÔºö{final_view}**ÔºàÂü∫‰∫éÂêÑÊ®°Âûã TrendAcc>0.5 ÁöÑÂä†ÊùÉÊäïÁ•®Ôºâ")

else:
    st.info("Âú®Â∑¶‰æßÈÄâÊã©ÂèÇÊï∞ÂêéÔºåÁÇπÂáª **‚ÄúüöÄ ÂºÄÂßãÈ¢ÑÊµã‚Äù**„ÄÇÁ§∫‰æã‰∫§ÊòìÂØπÔºö`BTC-USDT`ÔºåÂë®Êúü `1D`ÔºåËæìÂÖ•Á™óÂè£ 60ÔºåÈ¢ÑÊµã 15„ÄÇ")

with st.expander("ÊñπÊ≥ïËØ¥Êòé‰∏éÂÖçË¥£Â£∞Êòé"):
    st.markdown("""
- **Êï∞ÊçÆÊ∫ê**ÔºöOKX v5 ÂÖ¨ÂÖ±Ë°åÊÉÖ APIÔºàÂÖç KeyÔºâÔºåKÁ∫øÂë®ÊúüÔºö1D/4H/1HÔºõ
- **ÁâπÂæÅÂ∑•Á®ã**ÔºöOHLCV„ÄÅÊî∂ÁõäÁéá„ÄÅÂùáÁ∫øÔºàMA5/10/20Ôºâ„ÄÅÊ≥¢Âä®Áéá„ÄÅRSI„ÄÅOBVÔºõ
- **ÂΩí‰∏ÄÂåñ**ÔºöÁâπÂæÅ‰∏éÁõÆÊ†áÂàÜÂºÄ MinMaxScalerÔºõ
- **Êï∞ÊçÆÈõÜ**ÔºöÂõ∫ÂÆöËæìÂÖ•Á™óÂè£ÔºàÈªòËÆ§ 60 Ê†πÔºâÔºåËæìÂá∫Êú™Êù• `H` Ê≠•ÔºàÈªòËÆ§ 15ÔºâÔºõ
- **Ê®°Âûã**ÔºöLSTM / GRU / Transformer / TCNÔºõ
- **ËØÑ‰º∞**ÔºöRMSE„ÄÅMAE„ÄÅR¬≤„ÄÅË∂ãÂäøÂáÜÁ°ÆÁéáÔºõ
- **Âª∫ËÆÆ**ÔºöÂü∫‰∫éÈ¢ÑÊµãË∑ØÂæÑÊñúÁéá‰∏éË∂ãÂäøÂáÜÁ°ÆÁéáÔºà>0.55ÔºâÁöÑËßÑÂàôÂåñÊäïÁ•®Ôºå‰ªÖ‰æõÂèÇËÄÉÔºå**‰∏çÊûÑÊàêÊäïËµÑÂª∫ËÆÆ**„ÄÇ
""")
